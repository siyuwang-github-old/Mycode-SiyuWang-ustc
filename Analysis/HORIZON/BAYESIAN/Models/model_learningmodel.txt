model {
	# Modified by Siyu from Bob's code
    # This model has:
    #   Kalman filter inference
    #   Info bonus

    #   spatial bias is in this one and it can vary by 

    # no choice kernel
    # inference is constant across horizon and uncertainty but can vary by 
    # "condition".  Condition can be anything e.g. TMS or losses etc ...
    
    # two types of condition:
    #   * inference fixed - e.g. horizon, uncertainty - C1, nC1
    #   * inference varies - e.g. TMS, losses - C2, nCond

    # hyperpriors =========================================================

    # inference does not vary by condition 1, but can by condition 2
    for (j in 1:nCond) { # loop over condition 2
        # note, always use j to refer to condition 2
        a0_p[j] ~ dunif(0.1, 10) #dexp(0.001)
        b0_p[j] ~ dunif(0.5, 10) #dexp(0.0001)
        a_inf_p[j] ~ dunif(0.1, 10) #dexp(0.001)
        b_inf_p[j] ~ dunif(0.1, 10) #dexp(0.0001)
        mu0_mean_n[j] ~ dnorm(50, 0.005)
        mu0_sigma_p[j] ~ dgamma(1,0.001)
        mu0_tau[j] <- 1/mu0_sigma_p[j]
    }

    

 
    # information bonus and decision noise vary by condition 1 and condition 2
    for (i in 1:nHorizon){ # loop over horizon
        for (j in 1:nCond) { # loop over loss conditions

            AA_mean_n22[i,j] ~ dnorm(0, 0.0001)
            AA_sigma_p22[i,j] ~ dgamma(1,0.001)
            AA_tau[i,j] <- 1 / AA_sigma_p22[i,j]
        
            # BB_k_p22[i,j] ~ dexp(0.1)
            # BB_lambda_p22[i,j] ~ dexp(0.1)
            BB_k_p22[i,j] ~ dexp(1)
            BB_lambda_p22[i,j] ~ dexp(10)
            BB_mean[i,j] <- BB_k_p22[i,j] / BB_lambda_p22[i,j]

            SB_mean_n22[i,j] ~ dnorm(0, 0.0001)
            SB_sigma_p22[i,j] ~ dgamma(1,0.001)
            SB_tau[i,j] <- 1 / SB_sigma_p22[i,j]
        
        }
    }

    # priors ==============================================================
    for (s in 1:nSubject) { # loop over subjects
		
        # inference -------------------------------------------------------
        #for (j in 1:nCond) {
            # initial learning rate - so a0, b0 define alpha0 and note alpha0 is same for both bandits
            dum[s] ~ dbeta(a0_p[Cond[s]], b0_p[Cond[s]])
            alpha_start[s] <- dum[s]*0.999 # hack to prevent alpha_start == 1
            # asymptotic learning rate
            alpha_inf[s] ~ dbeta(a_inf_p[Cond[s]], b_inf_p[Cond[s]])

            # initial value
            mu0[s] ~ dnorm( mu0_mean_n[Cond[s]], mu0_tau[Cond[s]] )

            # compute alpha0 and alpha_d
            alpha0[s]  <- alpha_start[s] / (1 - alpha_start[s]) - alpha_inf[s]^2 / (1 - alpha_inf[s])
            alpha_d[s] <- alpha_inf[s]^2 / (1 - alpha_inf[s])
        #}
        
        # information bonus and decision noise ----------------------------
        for (i in 1:nHorizon) { # loop over horizon
        #    for (j in 1:nCond) { # loop over uncertainty condition

                # information bonus
                AA[s,i] ~ dnorm( AA_mean_n22[i,Cond[s]], AA_tau[i,Cond[s]] )
                # spatial bias
                SB[s,i] ~ dnorm( SB_mean_n22[i,Cond[s]], SB_tau[i,Cond[s]] )
                # decision noise
                BB[s,i] ~ dgamma( BB_k_p22[i,Cond[s]], BB_lambda_p22[i,Cond[s]] )
               
        #    }
        }

    }


    # subject level =======================================================
    for (s in 1:nSubject) { # loop over subjects
        for (g in 1:nTrial[s]) { # loop over games
            
            # inference model ---------------------------------------------
            
            # initialize stuff
            # learning rates 
            alpha1[s,g,1] <- alpha0[s]
            alpha2[s,g,1] <- alpha0[s]

            # values
            mu1[s,g,1] <- mu0[s]
            mu2[s,g,1] <- mu0[s]

            # information bonus and decision noise for this game depend on 
            # condition 1, C1, and condition 2, C2
            A[s,g] <- AA[s, horizon[s,g]]
            sigma_g[s,g] <- BB[s, horizon[s,g]]
            bias[s,g] <- SB[s, horizon[s,g]]


            # run inference model
            for (t in 1:nForcedTrials) { # loop over forced-choice trials

                # learning rates
                alpha1[s,g,t+1] <- ifelse( c[s,g,t] == 1, 1/( 1/(alpha1[s,g,t] + alpha_d[s]) + 1 ), 1/( 1/(alpha1[s,g,t] + alpha_d[s]) ) )
                alpha2[s,g,t+1] <- ifelse( c[s,g,t] == 2, 1/( 1/(alpha2[s,g,t] + alpha_d[s]) + 1 ), 1/( 1/(alpha2[s,g,t] + alpha_d[s]) ) )

                # update means for each bandit
                mu1[s,g,t+1] <- ifelse( c[s,g,t] == 1, mu1[s,g,t] + alpha1[s,g,t+1] * (r[s,g,t] - mu1[s,g,t]), mu1[s,g,t])
                mu2[s,g,t+1] <- ifelse( c[s,g,t] == 2, mu2[s,g,t] + alpha2[s,g,t+1] * (r[s,g,t] - mu2[s,g,t]), mu2[s,g,t])

            }

            # compute difference in values
            dQ[s,g] <- mu2[s,g,nForcedTrials+1] - mu1[s,g,nForcedTrials+1] + A[s,g] * dInfo[s,g] + bias[s,g]

            # choice probabilities
            p[s,g] <- 1 / ( 1 + exp( - dQ[s,g] / (sigma_g[s,g])))

            # choices
            c5[s,g] ~ dbern( p[s,g] )

        }
    }
    


    


}